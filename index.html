<!DOCTYPE html>
<html lang="es">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Asistente Visual Inteligente</title>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd"></script>
  <style>
    body {
      background-color: #0d1117;
      color: #fff;
      font-family: "Segoe UI", sans-serif;
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: center;
      height: 100vh;
      margin: 0;
    }
    video {
      border-radius: 16px;
      box-shadow: 0 0 20px rgba(0,0,0,0.5);
      width: 90%;
      max-width: 600px;
    }
    #status {
      margin-top: 20px;
      font-size: 1.3em;
      background: rgba(255,255,255,0.1);
      padding: 12px 20px;
      border-radius: 12px;
    }
  </style>
</head>
<body>
  <h2>üëÅ Asistente Visual Inteligente</h2>
  <video id="video" autoplay playsinline></video>
  <div id="status">Cargando modelo...</div>
  <button id="startButton" style="display: none; font-size: 1.2em; padding: 10px 20px; border-radius: 10px; cursor: pointer;">Iniciar Asistente</button>

  <script>
    const video = document.getElementById('video');
    const statusDiv = document.getElementById('status');
    let model;
    let lastSpokenMessage = "";
    let isDetecting = false; // Flag para controlar el bucle de detecci√≥n

    // --- Constantes para la detecci√≥n (mejor para ajustar) ---
    const MIN_AREA_CLOSE_OBJECT = 30000; // √Årea m√≠nima para considerar un objeto "cercano"
    const MIN_AREA_WALL = 200000; // √Årea m√≠nima para considerar una pared
    const WALL_Y_THRESHOLD = 150; // Posici√≥n Y m√°xima para ser considerada pared

    // Ocultar elementos al inicio para que el bot√≥n de inicio sea el protagonista
    video.style.display = 'none';
    statusDiv.style.display = 'none';

    // Activar voz del navegador
    function speak(message) {
      // Solo hablar si el mensaje es nuevo y no se est√° hablando ya
      if (window.speechSynthesis.speaking || message === lastSpokenMessage) return;

      const utterance = new SpeechSynthesisUtterance(message);
      utterance.lang = 'es-MX';
      window.speechSynthesis.speak(utterance);
      lastSpokenMessage = message;
      statusDiv.textContent = message;
    }

    // Acceso a la c√°mara
    async function setupCamera() {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({ video: { facingMode: 'environment' } });
        video.srcObject = stream;
        return new Promise(resolve => {
          video.onloadedmetadata = () => resolve(video);
        });
      } catch (error) {
        console.error("Error al acceder a la c√°mara:", error);
        statusDiv.textContent = "Error: No se pudo acceder a la c√°mara. Revisa los permisos.";
        statusDiv.style.display = 'block'; // Mostrar el error
        throw error; // Detener la ejecuci√≥n
      }
    }

    // Cargar modelo
    async function loadModel() {
      try {
        // Usamos el modelo 'lite_mobilenet_v2' que es mucho m√°s r√°pido en m√≥viles
        model = await cocoSsd.load({ base: 'lite_mobilenet_v2' });
        statusDiv.textContent = "Asistente listo ‚úÖ";
        detectFrame(); // Iniciar el bucle de detecci√≥n
      } catch (error) {
        console.error("Error al cargar el modelo:", error);
        statusDiv.textContent = "Error: No se pudo cargar el modelo de IA.";
        throw error;
      }
    }

    // L√≥gica de detecci√≥n
    async function detectFrame() {
      // Optimizaci√≥n: Si ya estamos detectando, no empezamos una nueva detecci√≥n.
      // Esto evita sobrecargar el tel√©fono.
      if (isDetecting || !model || video.paused || video.ended) {
        requestAnimationFrame(detectFrame);
        return;
      }

      isDetecting = true; // Marcamos que estamos empezando a detectar

      const predictions = await model.detect(video);

      let hasObstacle = false;
      let personClose = false;
      let vehicleClose = false;
      let wallDetected = false;
      let message = "Camino libre.";

      for (const obj of predictions) {
        const [x, y, width, height] = obj.bbox;
        const area = width * height;

        if (obj.class === 'person' && area > MIN_AREA_CLOSE_OBJECT) personClose = true;
        if ((obj.class === 'car' || obj.class === 'truck' || obj.class === 'bus') && area > MIN_AREA_CLOSE_OBJECT) vehicleClose = true;
        if (obj.class === 'bench' || obj.class === 'chair' || obj.class === 'bicycle') hasObstacle = true;

        // Detectar "pared" o "superficie cercana"
        if (area > MIN_AREA_WALL && y < WALL_Y_THRESHOLD) wallDetected = true;
      }

      if (wallDetected) message = "Pared al frente, det√©ngase.";
      else if (personClose) message = "Persona al frente, cuidado.";
      else if (vehicleClose) message = "Veh√≠culo cerca, precauci√≥n.";
      else if (hasObstacle) message = "Hay un objeto obstruyendo el camino.";
      else message = "Camino libre.";
      
      // Solo hablamos si el mensaje ha cambiado para no ser repetitivos
      if (message !== lastSpokenMessage) {
        speak(message);
      }

      isDetecting = false; // Marcamos que hemos terminado
      requestAnimationFrame(detectFrame); // Solicitamos el siguiente cuadro
    }

    // Funci√≥n principal que se ejecuta al pulsar el bot√≥n
    async function startAssistant() {
      startButton.style.display = 'none';
      video.style.display = 'block';
      statusDiv.style.display = 'block';
      statusDiv.textContent = 'Iniciando c√°mara...';
      await setupCamera();
      statusDiv.textContent = 'Cargando modelo...';
      await loadModel();
    }

    // Limpiar recursos al salir de la p√°gina para que la c√°mara no se quede encendida
    window.addEventListener('beforeunload', () => {
      if (video.srcObject) {
        video.srcObject.getTracks().forEach(track => track.stop());
        model = null; // Liberar el modelo
      }
      window.speechSynthesis.cancel();
    });

    // Mostrar el bot√≥n para que el usuario inicie la acci√≥n
    statusDiv.textContent = "Listo para iniciar.";
    startButton.style.display = 'block';
    startButton.addEventListener('click', () => {
      startAssistant().catch(err => console.error("No se pudo iniciar el asistente:", err));
    });
  </script>
</body>
</html>